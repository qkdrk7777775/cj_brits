{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(X):\n",
    "    std=np.apply_along_axis(np.nanstd,0,X)\n",
    "    mean=np.apply_along_axis(np.nanmean,0,X)\n",
    "    z=(X-mean)/std\n",
    "    return [z,mean,std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\" \n",
    "    자료 로드 부분 입력\n",
    "    - forward\n",
    "      - deltas\n",
    "      - masks\n",
    "      - values\n",
    "    \n",
    "    - backward\n",
    "      - deltas\n",
    "      - masks\n",
    "      - values\n",
    "      \n",
    "    \"\"\" \n",
    "    def __init__(self):\n",
    "        super(MyDataset,self).__init__()\n",
    "        \n",
    "        df=pd.read_csv('../NA_30per_QC2_gamak1 (1).csv',parse_dates=True,na_values='NaN')\n",
    "        df['date']=pd.to_datetime(df['date'])\n",
    "\n",
    "        #full date generate\n",
    "        df=df.set_index('date')\n",
    "\n",
    "        #year\n",
    "        df=df[:'2018-01-01']\n",
    "        temp=standardization(df)\n",
    "        df=temp[0]\n",
    "        self.mean_sd=temp[1],temp[2]\n",
    "        #dt_range = pd.date_range(min(df.index), max(df.index), freq='1H')\n",
    "        #na_date=set(dt_range)-set(df.index)\n",
    "        #df=pd.concat([df ,pd.DataFrame(columns=df.columns,index=na_date)],axis=0)\n",
    "        df=df.sort_index()\n",
    "        self.df=df\n",
    "        df_temp=df.copy()\n",
    "        df_temp=df_temp.fillna(method='ffill')\n",
    "        self.df_forwards=df_temp\n",
    "        \n",
    "        #forward\n",
    "        dic=dict()\n",
    "        for var in df.columns.values.tolist():\n",
    "            print(var)\n",
    "            temp=list()\n",
    "            temp.append(0)\n",
    "            for i in range(1,df.shape[0]):\n",
    "                temp_df=df[[var]][i:i+1]\n",
    "                if temp_df.notnull().values!=True:\n",
    "                    temp.append(i-temp[-1])\n",
    "                else:\n",
    "                    temp.append(temp[-1])\n",
    "            dic.update({var:temp})\n",
    "        \n",
    "        self.deltas=np.array(pd.DataFrame(dic))\n",
    "        #eval generate\n",
    "        df_temp=df.copy()\n",
    "        for seed in range(1,df.shape[1]):\n",
    "            np.random.seed(seed)\n",
    "            temp_idx=np.random.choice(df.shape[0],int(np.ceil(df.shape[0]*.1)))\n",
    "            df_temp[df_temp.columns[seed]][temp_idx]=np.nan\n",
    "        self.eval=df_temp\n",
    "        \n",
    "        #backward\n",
    "        df=df[::-1]\n",
    "        self.b_df=df\n",
    "        df_temp=df.copy()\n",
    "        df_temp=df_temp.fillna(method='ffill')\n",
    "        self.b_df_forwards=df_temp\n",
    "        dic=dict()\n",
    "        for var in df.columns.values.tolist():\n",
    "            print(var)\n",
    "            temp=list()\n",
    "            temp.append(0)\n",
    "            for i in range(1,df.shape[0]):\n",
    "                temp_df=df[[var]][i:i+1]\n",
    "                if temp_df.notnull().values!=True:\n",
    "                    temp.append(-(i-temp[-1]))\n",
    "                else:\n",
    "                    temp.append(temp[-1])\n",
    "            dic.update({var:temp})\n",
    "        self.back_deltas=np.array(pd.DataFrame(dic))\n",
    "\n",
    "        #eval generate\n",
    "        df_temp=df.copy()\n",
    "        for seed in range(1,df.shape[1]):\n",
    "            np.random.seed(seed)\n",
    "            temp_idx=np.random.choice(df.shape[0],int(np.ceil(df.shape[0]*.1)))\n",
    "            df_temp[df_temp.columns[seed]][temp_idx]=np.nan\n",
    "        self.b_eval=df_temp\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx>=50:\n",
    "            \n",
    "            data=dict()\n",
    "            data['forward']={'values':np.array(self.df)[(idx-50):(idx)],'masks':np.array(self.df.notnull()+0)[(idx-50):(idx)],\n",
    "                             'deltas':self.deltas[idx:(idx+50)],'forwards':np.array(self.df_forwards)[(idx-50):(idx)],\n",
    "                             'evals':np.array(self.eval)[(idx-50):(idx)],'eval_masks':np.array(self.eval.notnull()+0)[(idx-50):(idx)]}\n",
    "\n",
    "            data['backward']={'values':np.array(self.b_df)[(idx-50):(idx)],'masks':np.array(self.b_df.notnull()+0)[(idx-50):(idx)],\n",
    "                              'deltas':self.back_deltas[(idx-50):(idx)],'forwards':np.array(self.b_df_forwards)[(idx-50):(idx)],\n",
    "                             'evals':np.array(self.b_eval)[(idx-50):(idx)],'eval_masks':np.array(self.b_eval.notnull()+0)[(idx-50):(idx)]}\n",
    "            return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(recs):\n",
    "    if recs[0]!=None:\n",
    "        forward  = list(map(lambda x: x['forward'], recs))\n",
    "        backward = list(map(lambda x: x['backward'], recs))\n",
    "\n",
    "        def to_tensor_dict(recs):\n",
    "            values     =torch.FloatTensor(list(map(lambda r:r['values'],recs)))\n",
    "            masks      =torch.FloatTensor(list(map(lambda r:r['masks'], recs)))\n",
    "            deltas     =torch.FloatTensor(list(map(lambda r:r['deltas'],recs)))\n",
    "            evals      =torch.FloatTensor(list(map(lambda r:r['evals'], recs)))\n",
    "            eval_masks =torch.FloatTensor(list(map(lambda r:r['eval_masks'], recs)))\n",
    "\n",
    "            return {'values':values, 'masks': masks, 'deltas': deltas, 'evals': evals, 'eval_masks': eval_masks} \n",
    "\n",
    "        ret_dict = {'forward': to_tensor_dict(forward), 'backward': to_tensor_dict(backward)}\n",
    "        return ret_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mydata=MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def to_var(var):\n",
    "    if torch.is_tensor(var):\n",
    "        var = Variable(var)\n",
    "        if torch.cuda.is_available():\n",
    "            var = var.cuda()\n",
    "        return var\n",
    "    if isinstance(var, int) or isinstance(var, float) or isinstance(var, str):\n",
    "        return var\n",
    "    if isinstance(var, dict):\n",
    "        for key in var:\n",
    "            var[key] = to_var(var[key])\n",
    "        return var\n",
    "    if isinstance(var, list):\n",
    "        var = map(lambda x: to_var(x), var)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('/home/ducj2/data/do_bottom/models')\n",
    "os.chdir('../')\n",
    "import models\n",
    "import utils\n",
    "import sys\n",
    "sys.path.append('./models')\n",
    "from models import brits_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter=torch.utils.data.DataLoader(Mydata,batch_size=32,collate_fn=collate_fn,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    evals = []\n",
    "    imputations = []\n",
    "\n",
    "    save_impute = []\n",
    "    save_label = []\n",
    "\n",
    "    for idx, data in enumerate(val_iter):\n",
    "        print(idx)\n",
    "        if data!=None:\n",
    "            data = to_var(data)\n",
    "            ret = model.run_on_batch(data, None)\n",
    "\n",
    "            # save the imputation results which is used to test the improvement of traditional methods with imputed values\n",
    "            save_impute.append(ret['imputations'].data.cpu().numpy())\n",
    "\n",
    "            pred = ret['predictions'].data.cpu().numpy()\n",
    "\n",
    "            eval_masks = ret['eval_masks'].data.cpu().numpy()\n",
    "            eval_ = ret['evals'].data.cpu().numpy()\n",
    "            imputation = ret['imputations'].data.cpu().numpy()\n",
    "\n",
    "            evals += eval_[np.where(eval_masks == 1)].tolist()\n",
    "            imputations += imputation[np.where(eval_masks == 1)].tolist()\n",
    "\n",
    "            # collect test label & prediction\n",
    "            pred = pred\n",
    "            preds += pred.tolist()\n",
    "    preds = np.asarray(preds)\n",
    "    print('AUC {}'.format(metrics.roc_auc_score(labels, preds)))\n",
    "\n",
    "    evals = np.asarray(evals)\n",
    "    imputations = np.asarray(imputations)\n",
    "\n",
    "    print('MAE', np.abs(evals - imputations).mean())\n",
    "    print('MRE', np.abs(evals - imputations).sum() / np.abs(evals).sum())\n",
    "    save_impute = np.concatenate(save_impute, axis=0)\n",
    "    np.save('./result/{}_data'.format(args.model), save_impute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Progress epoch 0, 0.88%, average loss 23.83338673909505\n",
      " Progress epoch 0, 1.17%, average loss 34.847957611083984\n",
      " Progress epoch 0, 1.47%, average loss 41.80503540039062\n",
      " Progress epoch 0, 1.76%, average loss 46.08460235595703\n",
      " Progress epoch 0, 2.05%, average loss 48.89100211007254\n",
      " Progress epoch 0, 2.35%, average loss 50.759544372558594\n",
      " Progress epoch 0, 2.64%, average loss 52.42593299018012\n",
      " Progress epoch 0, 2.93%, average loss 54.15531463623047\n",
      " Progress epoch 0, 3.23%, average loss 56.47785741632635\n",
      " Progress epoch 0, 3.52%, average loss 57.811875661214195\n",
      " Progress epoch 0, 3.81%, average loss 57.74129398052509\n",
      " Progress epoch 0, 4.11%, average loss 57.98536845615932\n",
      " Progress epoch 0, 4.40%, average loss 58.96576792399089\n",
      " Progress epoch 0, 4.69%, average loss 60.63907337188721\n",
      " Progress epoch 0, 4.99%, average loss 61.14406047147863\n",
      " Progress epoch 0, 5.28%, average loss 60.31928740607368\n",
      " Progress epoch 0, 5.57%, average loss 59.34058841906096\n",
      " Progress epoch 0, 5.87%, average loss 58.622599601745605\n",
      " Progress epoch 0, 6.16%, average loss 58.2175171261742\n",
      " Progress epoch 0, 6.45%, average loss 57.98130174116655\n",
      " Progress epoch 0, 6.74%, average loss 57.764070427936055\n",
      " Progress epoch 0, 7.04%, average loss 57.54386679331461\n",
      " Progress epoch 0, 7.33%, average loss 57.353738708496095\n",
      " Progress epoch 0, 7.62%, average loss 57.16903877258301\n",
      " Progress epoch 0, 7.92%, average loss 57.169435571741175\n",
      " Progress epoch 0, 8.21%, average loss 56.787107058933806\n",
      " Progress epoch 0, 8.50%, average loss 56.33849453103954\n",
      " Progress epoch 0, 8.80%, average loss 56.33400001525879\n",
      " Progress epoch 0, 9.09%, average loss 56.748514113887666\n",
      " Progress epoch 0, 9.38%, average loss 57.32340967655182\n",
      " Progress epoch 0, 9.68%, average loss 57.67453268802527\n",
      " Progress epoch 0, 9.97%, average loss 57.809960533590875\n",
      " Progress epoch 0, 10.26%, average loss 57.84002870832171\n",
      " Progress epoch 0, 10.56%, average loss 57.84976885053847\n",
      " Progress epoch 0, 10.85%, average loss 57.81078019013276\n",
      " Progress epoch 0, 11.14%, average loss 57.78837826377467\n",
      " Progress epoch 0, 11.44%, average loss 57.8292727348132\n",
      " Progress epoch 0, 11.73%, average loss 57.93487234115601\n",
      " Progress epoch 0, 12.02%, average loss 57.90803592961009\n",
      " Progress epoch 0, 12.32%, average loss 57.88388215927851\n",
      " Progress epoch 0, 12.61%, average loss 57.73546112415402\n",
      " Progress epoch 0, 12.90%, average loss 57.63677605715665\n",
      " Progress epoch 0, 13.20%, average loss 57.70154910617404\n",
      " Progress epoch 0, 13.49%, average loss 57.78185910763948\n",
      " Progress epoch 0, 13.78%, average loss 57.838751447961684\n",
      " Progress epoch 0, 14.08%, average loss 57.91205461819967\n",
      " Progress epoch 0, 14.37%, average loss 57.93829836164202\n",
      " Progress epoch 0, 14.66%, average loss 57.92375869750977\n",
      " Progress epoch 0, 14.96%, average loss 58.06752792059206\n",
      " Progress epoch 0, 15.25%, average loss 58.26593457735502\n",
      " Progress epoch 0, 15.54%, average loss 58.49378017209611\n",
      " Progress epoch 0, 15.84%, average loss 58.899347658510564\n",
      " Progress epoch 0, 16.13%, average loss 59.29972118030895\n",
      " Progress epoch 0, 16.42%, average loss 59.69702707018171\n",
      " Progress epoch 0, 16.72%, average loss 60.05685250800953\n",
      " Progress epoch 0, 17.01%, average loss 60.27592599803004\n",
      " Progress epoch 0, 17.30%, average loss 60.33628605988066\n",
      " Progress epoch 0, 17.60%, average loss 60.27802448272705\n",
      " Progress epoch 0, 17.89%, average loss 60.12231270211642\n",
      " Progress epoch 0, 18.18%, average loss 59.92748703495149\n",
      " Progress epoch 0, 18.48%, average loss 59.76085923210023\n",
      " Progress epoch 0, 18.77%, average loss 59.58123219013214\n",
      " Progress epoch 0, 19.06%, average loss 59.400077115572415\n",
      " Progress epoch 0, 19.35%, average loss 59.247319250395805\n",
      " Progress epoch 0, 19.65%, average loss 59.150462022468226\n",
      " Progress epoch 0, 19.94%, average loss 59.09550593881046\n",
      " Progress epoch 0, 20.23%, average loss 59.05663520702417\n",
      " Progress epoch 0, 20.53%, average loss 58.98180127825056\n",
      " Progress epoch 0, 20.82%, average loss 58.86335238604478\n",
      " Progress epoch 0, 21.11%, average loss 58.75241332583957\n",
      " Progress epoch 0, 21.41%, average loss 58.86562598241519\n",
      " Progress epoch 0, 21.70%, average loss 59.07197137781092\n",
      " Progress epoch 0, 21.99%, average loss 59.28901194254557\n",
      " Progress epoch 0, 22.29%, average loss 59.50928376850329\n",
      " Progress epoch 0, 22.58%, average loss 59.693174981451655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-68496ec19362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrun_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.5/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter/lib/python3.5/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-396a11b29af8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             data['forward']={'values':np.array(self.df)[(idx-50):(idx)],'masks':np.array(self.df.notnull()+0)[(idx-50):(idx)],\n\u001b[0m\u001b[1;32m     95\u001b[0m                              \u001b[0;34m'deltas'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'forwards'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_forwards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                              'evals':np.array(self.eval)[(idx-50):(idx)],'eval_masks':np.array(self.eval.notnull()+0)[(idx-50):(idx)]}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model = getattr(models, args.model).Model(108, 0.3, 1)\n",
    "model= brits_i.Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "data_iter=torch.utils.data.DataLoader(Mydata,batch_size=32,collate_fn=collate_fn,shuffle=False)\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    run_loss = 0.0\n",
    "    for idx, data in enumerate(data_iter):\n",
    "        if data!=None:\n",
    "            data = to_var(data)\n",
    "            ret = model.run_on_batch(data, optimizer)\n",
    "            run_loss += ret['loss'].item()\n",
    "            print ('\\r Progress epoch {}, {:.2f}%, average loss {}'.format(epoch, (idx + 1) * 100.0 / len(data_iter), run_loss / (idx + 1.0)))\n",
    "    evaluate(model, data_iter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
